#!/bin/bash

subj_id=SUBJECTID
ses_id=SESID
temp_dir=TEMPDIR
data_bucket=BUCKET
run_dir=RUNDIR
singularity=`which singularity`

BIDS_DIR=${temp_dir}/BIDS
SUB_BIDS_DIR=${temp_dir}/BIDS/sub-${subj_id}
SES_BIDS_DIR=${temp_dir}/BIDS/sub-${subj_id}/ses-${ses_id}
EXT_ASEG_DIR=${temp_dir}/ext_aseg
OUTPUT_DIR=${temp_dir}/processed/dcan-infant-pipeline
s3_aseg=${data_bucket}/derivatives/manual_segmentations/sub-${subj_id}/ses-${ses_id}/aseg_acpc.nii.gz

# create these folders on global scratch if they don't exist 
if [ ! -d ${BIDS_DIR} ]; then
	mkdir -p ${BIDS_DIR}
fi
if [ ! -d ${SES_BIDS_DIR} ]; then
	mkdir -p ${SES_BIDS_DIR}
fi
if [ ! -d ${EXT_ASEG_DIR} ]; then
	mkdir -p ${EXT_ASEG_DIR}
fi
if [ ! -d ${OUTPUT_DIR} ]; then
	mkdir -p ${OUTPUT_DIR}
fi

# pull down needed data and files from BIDS bucket
# BIDS and external segmentation if needed
aseg_filename=sub-${subj_id}_ses-${ses_id}_aseg_acpc.nii.gz

s3cmd get ${data_bucket}/sub-${subj_id}/ses-${ses_id} ${SUB_BIDS_DIR}/ --recursive -v
s3cmd get ${s3_aseg} ${EXT_ASEG_DIR}/${aseg_filename} --recursive -v

if [ ! -e ${BIDS_DIR}/dataset_description.json ]; then
	cp ${run_dir}/dataset_description.json ${BIDS_DIR}
fi
if [ ! -e ${BIDS_DIR}/participants.tsv ]; then
	s3cmd get ${data_bucket}/participants.tsv ${BIDS_DIR} -v 
fi

infant_abcd_bids_pipeline=/home/faird/shared/code/internal/pipelines/DCAN-infant-BIDS/infant-abcd-bids-pipeline_latest_02202023a.sif
echo ${infant_abcd_bids_pipeline}
FS_LICENSE=/home/faird/shared/code/external/utilities/freesurfer_license
HNM=ROI_IPS
NCPUS=12

lic_loc=/opt/freesurfer/license.txt

singularity run --cleanenv \
-e \
-B ${BIDS_DIR}:/bids_input \
-B ${OUTPUT_DIR}:/output \
-B ${EXT_ASEG_DIR}:/aseg_dir \
-B ${FS_LICENSE}/license.txt:${lic_loc} \
${infant_abcd_bids_pipeline} \
--freesurfer-license ${lic_loc} \
--participant-label ${subj_id} \
--session-id ${ses_id} \
--hyper-normalization-method ${HNM} \
--ncpus ${NCPUS} \
--aseg /aseg_dir/${aseg_filename} \
--anat-only \
--no-crop \
/bids_input /output

# when using global scratch, delete BIDS at end 
#rm -fr ${SES_BIDS_DIR}

#push processed outputs to bucket
s3cmd sync -F --recursive -v ${OUTPUT_DIR}/sub-${subj_id}/ses-${ses_id}/ ${data_bucket}/processed_w_man_corr_aseg/dcan-infant-pipeline/sub-${subj_id}/ses-${ses_id}/


# --atropos-mask-method NONE \
# --no-crop \
# --dcmethod NONE \