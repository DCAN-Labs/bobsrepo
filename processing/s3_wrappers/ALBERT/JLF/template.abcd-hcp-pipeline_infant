#!/bin/bash

subj_id=SUBJECTID
ses_id=SESID
temp_dir=TEMPDIR
data_bucket=BUCKET
run_dir=RUNDIR
singularity=`which singularity`

BIDS_DIR=${temp_dir}/BIDS
SUB_BIDS_DIR=${temp_dir}/BIDS/sub-${subj_id}
SES_BIDS_DIR=${temp_dir}/BIDS/sub-${subj_id}/ses-${ses_id}
OUTPUT_DIR=${temp_dir}/processed/dcan-infant-pipeline
#JLF_TEMPLATES_DIR=${temp_dir}/jlf_templates

# create these folders on global scratch if they don't exist 
# if [ ! -d ${JLF_TEMPLATES_DIR} ]; then
# 	mkdir -p ${JLF_TEMPLATES_DIR}
# fi
if [ ! -d ${BIDS_DIR} ]; then
	mkdir -p ${BIDS_DIR}
fi
if [ ! -d ${SES_BIDS_DIR} ]; then
	mkdir -p ${SES_BIDS_DIR}
fi
if [ ! -d ${OUTPUT_DIR} ]; then
	mkdir -p ${OUTPUT_DIR}
fi

# pull down needed data and files from BIDS bucket
s3cmd get ${data_bucket}/sub-${subj_id}/ses-${ses_id} ${SUB_BIDS_DIR}/ --recursive -v
#s3cmd get ${data_bucket}/updated_JLF_templates/${ses_id}/ ${JLF_TEMPLATES_DIR}/  --recursive -v

if [ ! -e ${BIDS_DIR}/dataset_description.json ]; then
	cp ${run_dir}/dataset_description.json ${BIDS_DIR}
fi
if [ ! -e ${BIDS_DIR}/participants.tsv ]; then
	s3cmd get ${data_bucket}/participants.tsv ${BIDS_DIR} -v 
fi

infant_abcd_bids_pipeline=/home/faird/shared/code/internal/pipelines/DCAN-infant-BIDS/infant-abcd-bids-pipeline_latest_02202023a.sif
echo ${infant_abcd_bids_pipeline}

JLF_TEMPLATES_DIR=/home/feczk001/shared/projects/BOBSRepo/PROC/updated_JLF_templates/0mo

FS_LICENSE=/home/faird/shared/code/external/utilities/freesurfer_license
HNM=ROI_IPS
NCPUS=12

lic_loc=/opt/freesurfer/license.txt

singularity run --cleanenv \
-e \
-B ${BIDS_DIR}:/bids_input \
-B ${OUTPUT_DIR}:/output \
-B ${JLF_TEMPLATES_DIR}:/jlf_templates \
-B ${FS_LICENSE}/license.txt:${lic_loc} \
${infant_abcd_bids_pipeline} \
--freesurfer-license ${lic_loc} \
--participant-label ${subj_id} \
--session-id ${ses_id} \
--hyper-normalization-method ${HNM} \
--ncpus ${NCPUS} \
--multi-template-dir /jlf_templates \
--anat-only \
--no-crop \
--atropos-mask-method NONE \
/bids_input /output

# when using global scratch, delete BIDS at end 
#rm -fr ${SES_BIDS_DIR}

#push processed outputs to bucket
s3cmd sync -F --recursive -v ${OUTPUT_DIR}/sub-${subj_id}/ses-${ses_id}/ ${data_bucket}/processed_JLF/dcan-infant-pipeline/sub-${subj_id}/ses-${ses_id}/


# --atropos-mask-method NONE \
# --no-crop \
# --dcmethod NONE \